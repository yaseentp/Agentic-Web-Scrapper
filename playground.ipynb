{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51448240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from utils.dataset import capture_screenshots_async\n",
    "from utils.dataset import capture_screenshots_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b39740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./data/Price_Normalization_Dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ecacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = await capture_screenshots_async(data)\n",
    "\n",
    "cols_to_fix = [\"Explictness\",\"Step1_desired_unit\", \"Step1_desired_value\"]\n",
    "\n",
    "dataset[cols_to_fix] = dataset[cols_to_fix].where(\n",
    "    pd.notna(dataset[cols_to_fix]),\n",
    "    None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69df2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_excel(\"./dataset/dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859c9a8",
   "metadata": {},
   "source": [
    "### Pydantic model and LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823337e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydantic models\n",
    "\n",
    "#output\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Literal, List\n",
    "\n",
    "\n",
    "class Evidence(BaseModel):\n",
    "    source: Literal[\"price_label\", \"specification_table\", \"more_information\",\"calculator box\"]\n",
    "    text: str\n",
    "\n",
    "\n",
    "class Quantity(BaseModel):\n",
    "    value: float\n",
    "    unit: str\n",
    "\n",
    "\n",
    "class Step1Output(BaseModel):\n",
    "    priced_quantity: Optional[Quantity]\n",
    "    explicitness: Literal[\"direct\", \"indirect\", \"none\"]\n",
    "    confidence: float\n",
    "    evidence: List[Evidence]\n",
    "    notes: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a pricing quantity inference engine.\n",
    "\n",
    "Your task is to identify the quantity that the listed price applies to,\n",
    "using ONLY explicit seller-visible information from the webpage.\n",
    "\n",
    "Allowed evidence:\n",
    "- Price labels or text directly adjacent to the price\n",
    "- Specification tables\n",
    "- Explicit product labels stating quantity or unit\n",
    "\n",
    "Do NOT:\n",
    "- use comments\n",
    "- infer from typical product sizes\n",
    "- assume standard lengths\n",
    "- perform unit conversion\n",
    "- perform calculations\n",
    "- if unit is not visible do not infer or assume any unit\n",
    "- treat cart quantity selectors, add-to-cart counters, or default quantity values (e.g. \"Quantity: 1\") as a pricing unit.\n",
    "\n",
    "Do:\n",
    "- Unit must be in full from\n",
    "- Convert millimeter to meters\n",
    "- If unit is not visible directlty adjacent to main price label set explicitness to \"indirect\"\n",
    "- If no standard unit (length, weight, area, volume, pack size) is explicitly stated,\n",
    "set explicitness to \"none\".\n",
    "\n",
    "Return ONLY a JSON object that matches the provided schema.\n",
    "Billing accuracy is required.\"\"\"\n",
    "\n",
    "\n",
    "USER_PROMPT = 'screenshot : '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c2d0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05f719b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAIError\n",
    "\n",
    "# Initialize model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "structure_model = model.with_structured_output(Step1Output)\n",
    "\n",
    "\n",
    "image_path = \"/Users/yaseen/Desktop/Gordian Agentic Web Scrapper/screenshots/case_0.png\"\n",
    "\n",
    "\n",
    "async def infer_from_webpage(image_path : str) -> Step1Output:\n",
    "\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "            human_msg = HumanMessage([\n",
    "                    {\"type\": \"text\", \"text\": USER_PROMPT},\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"base64\": encoded_image,\n",
    "                        \"mime_type\": \"image/png\",\n",
    "                    },\n",
    "                ])\n",
    "            \n",
    "            system_msg = SystemMessage(SYSTEM_PROMPT)\n",
    "\n",
    "            response = await structure_model.ainvoke([system_msg,human_msg])\n",
    "\n",
    "            return response\n",
    "        \n",
    "    except OpenAIError as e:\n",
    "        raise RuntimeError(f\"OpenAI API error: {e}\") from e\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Inference failed: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c42588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test infer_from_webpage\n",
    "response = await infer_from_webpage(image_path)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44d1e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "async def run_step1_on_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    semaphore_limit: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Output\"] = None\n",
    "\n",
    "    semaphore = asyncio.Semaphore(semaphore_limit)\n",
    "\n",
    "\n",
    "    async def run_one(index, row):\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                output: Step1Output = await infer_from_webpage(\n",
    "                    image_path=row[\"img_path\"],\n",
    "                )\n",
    "                return index, output.model_dump_json()\n",
    "\n",
    "            except Exception as e:\n",
    "                # Store error as string (important for dataset audit)\n",
    "                return index, f\"ERROR: {str(e)}\"\n",
    "\n",
    "    tasks = [\n",
    "        run_one(idx, row)\n",
    "        for idx, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for idx, result in results:\n",
    "        df.at[idx, \"Output\"] = result\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff84f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sample\n",
    "#sample = dataset.iloc[[1],:]\n",
    "\n",
    "dataset = pd.read_excel(\"./dataset/dataset.xlsx\")\n",
    "\n",
    "result_df = await run_step1_on_dataset(dataset[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "570164db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_excel('./result/result5.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a76856",
   "metadata": {},
   "source": [
    "# prompt improvment\n",
    "1. convert mm to m\n",
    "2. include calculator box / custome calculator table in evidenece source\n",
    "3. concrete the 'none' case, examples of this cases ouptut assume some unit ; it should not be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c5531",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f42e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c4679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476308ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later required\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "STEP1_TEXT_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"extracted_price\", \"product_description\"],\n",
    "    template=(\n",
    "        \"\"\"Extracted price: {extracted_price}\n",
    "        Product description: {product_description}\n",
    "        screenshot : \"\"\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP1_TEXT_PROMPT.format(extracted_price=5,product_description=\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
